## ESP32-S3 Voice Assistant over WebSocket (I2S mic -> FastAPI -> TTS back to speaker)

Build a low-latency voice assistant with an ESP32-S3 streaming I2S microphone audio to a FastAPI server over WebSocket. The server performs STT (ZipFormer via sherpa-onnx), LLM response generation (Gemini), and TTS (ZipVoice), then streams the synthesized WAV back for playback on the ESP32-S3 speaker. Optional on-device VAD helps the ESP32 decide when to start/stop sending audio.

This repo contains the Python server and AI pipeline; your ESP32-S3 firmware streams audio and plays back the response.


## What’s inside

- FastAPI server with a single WebSocket endpoint at `/ws`
- STT: ZipFormer (sherpa-onnx) running offline on CPU
- LLM: Google Gemini with simple RAG and conversation history
- TTS: ZipVoice (local inference) to generate WAV responses
- Binary streaming both ways: ESP32 sends raw PCM; server returns WAV bytes then a `TTS_END` signal

Key files:
- `main.py` – WebSocket server, audio buffering, stream-back, end-of-utterance via timeout
- `modules/stt.py` – STT using sherpa-onnx ZipFormer
- `modules/llm.py` – Gemini client, simple RAG, chat history
- `modules/tts.py` – ZipVoice inference wrapper, emits WAV
- `settings/*.py` – Paths and configuration for STT, TTS, and LLM


## Architecture

1) ESP32-S3 captures mono PCM from an I2S microphone at 16 kHz, 16-bit, little-endian.
2) ESP32-S3 opens a WebSocket to `ws://<server>:8000/ws` and streams PCM chunks (e.g., 512–2048 bytes each) while VAD says “speaking”.
3) Server buffers audio until a brief silence (timeout) indicates end-of-utterance, saves a WAV, and runs:
   - STT → text
   - LLM → reply text
   - TTS → reply.wav
4) Server streams `reply.wav` bytes back to the ESP32-S3, then sends a text frame `TTS_END` to signal playback completion.
5) ESP32-S3 plays audio via I2S to the speaker and returns to listening.


## Requirements

- OS: Linux/macOS/Windows
- Python: 3.10+ recommended
- CPU is fine; GPU not required

Python packages (see `requirements.txt`):
- `fastapi`, `uvicorn` (install via extras below), `sherpa-onnx`, `google-genai`, `soundfile`, `numpy`
- ZipVoice package is included in `ZipVoice/` and invoked as a module

Models expected:
- STT (ZipFormer ONNX): place in `models/ZipFormer/`
  - `tokens.txt`, `encoder-*.onnx`, `decoder-*.onnx`, `joiner-*.onnx`
- TTS (ZipVoice): place in `models/ZipVoice/` (e.g., `zipvoice.pt`, `tokens.txt`, `model.json`)


## Setup

1) Create and activate a virtual environment

```bash
python3 -m venv .venv
source .venv/bin/activate
```

2) Install Python dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt fastapi uvicorn
```

3) Put models in place

- Copy your ZipFormer ONNX files into `models/ZipFormer/`
- Copy your ZipVoice checkpoint and assets into `models/ZipVoice/`

4) Configure LLM API key

```bash
export GEMINI_API_KEY="<your_gemini_api_key>"
```

Optionally, adjust settings in:
- `settings/stt_settings.py` (sample rate, model dir)
- `settings/tts_settings.py` (ZipVoice path, model dir)
- `settings/llm_settings.py` (model name, RAG directory, temperature)


## Run the server

Start the FastAPI app with Uvicorn:

```bash
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

Health check:

- `GET /` → `{ "status": "Voice Assistant Server is running" }`


## WebSocket protocol (ESP32-S3 <-> server)

Endpoint: `ws://<server-ip>:8000/ws`

Audio format to send from ESP32-S3:
- Sample rate: 16000 Hz
- Channels: mono
- Bit depth: 16-bit signed PCM (little-endian)
- Chunk size: 512–2048 bytes per frame is typical; match your I2S DMA buffer

Stream lifecycle:
1) ESP32 connects and sends binary audio frames while VAD indicates speech is ongoing.
2) If ESP32 stops sending for ~0.7s, the server assumes end-of-utterance and processes.
3) Server streams back binary frames that together form a WAV file (includes standard WAV header) generated by TTS.
4) After sending the last byte, server sends a text frame `TTS_END` to signal completion.
5) ESP32 should then return to capture mode and be ready to send the next utterance.

Important notes for playback on ESP32:
- The server returns a complete WAV file stream (header + data). You can:
  - Parse the WAV header to get sample rate/format and configure I2S accordingly; then play the data section, or
  - Skip the first 44 bytes (typical PCM WAV header) if you already know the format and it matches your I2S output config.
- ZipVoice’s output sample rate may not be 16 kHz for all models. It’s best to read the WAV header on-device and set I2S TX sample rate to match.


## ESP32-S3 firmware guidance

I2S mic capture (RX):
- Use I2S in PDM or standard I2S mode depending on your mic (e.g., SPH0645/SICS). Configure mono, 16 kHz, 16-bit.
- DMA buffers sized to deliver ~10–40 ms per chunk (e.g., 160–640 samples = 320–1280 bytes at 16-bit mono).

I2S speaker playback (TX):
- Configure the TX rate dynamically from the WAV header returned by the server.
- If output is louder/softer than expected, scale samples or adjust DAC gain/I2S amplifier chain.

WebSocket client:
- Use `esp_websocket_client` (ESP-IDF) to connect with `ws://<server-ip>:8000/ws`.
- Send binary frames only for audio; do not mix text frames during capture.
- While receiving, treat frames as binary audio until a text frame `TTS_END` arrives.


## On-device VAD (ESP32-S3)

Goal: Start sending when speech begins; stop when silence is detected. A lightweight approach works well on ESP32-S3 (R16N8: 16 MB flash, 8 MB PSRAM).

Recommended simple VAD:
- Short-term energy (RMS) + zero-crossing rate (ZCR) with a noise floor calibration.
- Flow:
  1) On boot, capture ~500 ms of ambient audio to estimate noise RMS: $R_0$.
  2) For each frame (e.g., 20 ms = 320 samples at 16 kHz), compute RMS $R$ and ZCR.
  3) Speech start if $R > T_{start} \cdot R_0$ for N consecutive frames (e.g., $T_{start}=3.0$, $N=3$) and ZCR within human speech range.
  4) Speech end if $R < T_{stop} \cdot R_0$ for M consecutive frames (e.g., $T_{stop}=1.5$, $M=8$) or max utterance length reached.
  5) Optional: adaptive $R_0$ update during long silence.

Why this works:
- It’s CPU-light (just sums and compares) and robust enough for quiet rooms. For noisy rooms, consider adding a simple DC removal + 1st-order HPF and adjust thresholds.

Implementation tips (ESP-IDF pseudocode):
```c
// 16 kHz, 16-bit PCM samples in int16_t buf[frame_samples]
float rms(int16_t* buf, int n) {
	long long acc = 0;
	for (int i = 0; i < n; ++i) { int32_t s = buf[i]; acc += (long long)s * s; }
	return sqrtf((float)acc / (float)n);
}

bool vad_process(int16_t* buf, int n, float* noise_rms,
				 int* speech_cnt, int* silence_cnt,
				 bool* in_speech) {
	const float Tstart = 3.0f;  // start threshold multiple
	const float Tstop  = 1.5f;  // stop threshold multiple
	float r = rms(buf, n);
	float nr = *noise_rms;

	if (!*in_speech) {
		if (r > Tstart * nr) {
			if (++(*speech_cnt) >= 3) { *in_speech = true; *silence_cnt = 0; }
		} else {
			*speech_cnt = 0; // reset
			// slowly adapt noise floor
			*noise_rms = 0.99f * nr + 0.01f * r;
		}
	} else { // in speech
		if (r < Tstop * nr) {
			if (++(*silence_cnt) >= 8) { *in_speech = false; *speech_cnt = 0; return true; }
		} else {
			*silence_cnt = 0;
		}
	}
	return false; // not ended
}
```

If you prefer a proven algorithm, consider porting WebRTC VAD; the ESP32-S3 has enough PSRAM, but it’s more complex than the simple RMS method above.


## End-to-end timing and chunking

- Server side uses a silence timeout of ~0.7 s to detect end-of-utterance (see `AUDIO_TIMEOUT` in `main.py`).
- Smaller ESP32 chunk sizes reduce latency but increase overhead. Start with 20 ms frames (320 samples = 640 bytes) and adjust.
- For faster TTS, keep `DEFAULT_PROMPT_TEXT` short and pre-warm the pipeline (server initializes all models on startup).


## Troubleshooting

- STT model not found: verify `models/ZipFormer/` contains `tokens.txt`, `encoder*.onnx`, `decoder*.onnx`, `joiner*.onnx` (see `settings/stt_settings.py`).
- TTS fails: ensure ZipVoice checkpoint exists in `models/ZipVoice/` and `ZipVoice/` code is present (see `settings/tts_settings.py`). Check logs for the subprocess command that ran.
- LLM error: set `GEMINI_API_KEY` and confirm network access. You can disable thinking or reduce token limits in `settings/llm_settings.py`.
- Audio playback artifacts: parse WAV header on ESP32 and set I2S TX rate to match the file; avoid assuming 16 kHz on output.
- High latency: use smaller PCM chunks, tune VAD thresholds, and ensure server CPU scaling governor allows burst performance.


## Roadmap

- Server-side VAD to complement on-device VAD
- Optional streaming STT for partial results
- Gzip or ADPCM compression over WebSocket for slower links
- TLS (wss://) and auth tokens for production


## License

This repository contains third-party components (ZipVoice) with their own licenses. See `ZipVoice/LICENSE`. Your own code is under your chosen license; add a LICENSE file at the root if desired.

